---
slug: "tidync"
title: Using the tidync package for NetCDF data
package_version: 0.2.1
authors:
  - Michael Sumner
date: 2019-06-18
categories: blog
topicid:
tags:
- Software Peer Review
- R
- community
- software
- packages
- tidync
- NetCDF
- array
- tidyverse
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidync)
```


The [tidyverse](https://www.tidyverse.org/) has had an enormous impact on the use of R by dictating a strict approach to *variables* and *observations*. This very general insight can be used for any form of data but when it comes to large data obviously we can't store everything in one table. 

There is a tension between the **tidyverse** and **scientific array data** that comes down to data storage, and the intermediate forms used to get between one form and another. 

The tidync package provides a compromise position, allowing efficient array slicing by dimension coordinate or index, and delaying any data-reading until the output format is chosen. In particular tidync exists in order to reduce the amount of plumbing code required to get to the data, and allows an interactive way to convert between coordinate spaces and index spaces. 

To follow along with the code below requires all of the following packages. 

```R
install.packages(c("tidync", "maps", "stars", "ggplot2", "devtools", 
                   "stars", "RNetCDF", "raster", "dplyr"))
devtools::install_github("hypertidy/ncmeta")
                   
                   
```



## NetCDF

NetCDF is a very widely used file format for storing array-based data as
*variables*. The **space** occupied by a **variable** is defined by its
**dimensions** and their metadata. Dimensions are by definition
*one-dimensional* (i.e. an atomic vector in R of length 1 or more), an array with coordinate metadata, units,
type and interpretation. The **space** of a variable is defined as one or more
of the dimensions in the file. A given variable won't necessarily use all the
available dimensions and no dimensions are mandatory or particularly special.

NetCDF is very general, but it's quite common to see subcultures that rally around the way their particular domain's data are used and stored without encompassing very many other valid ways of using NetCDF. Tidync really tries to be as general as possible, sacrificing high level interpretations for lower-level control. 

## Tidync limitations

There are some limitations, specific to the tidync R package that are unrelated to the capabilities of the latest NetCDF library. 

No groups, a group can be specified by providing the group-within-a-source *as a source*. 

No compound types. 

No attribute metadata, coordinates of 1D axes are stored as *transform tables*, but coordinates of pairs (or higher sets) of axes are not explicitly linked to their array data.  

Curvilinear coordinates are not automatically expanded, this is because they exist (usually) on a different grid to the active one. 

## Different kinds of NetCDF

NetCDF can be used to store *raster data*, and very commonly data is provided as a global grid of scientific data, here a snapshot of global ocean surface temperature generated by blending remote sensing, local observations and physical model output. 

There's an example file "reduced.nc" in the `stars` package, derived from the daily [OISSTV2 product](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html). 

We will explore the description of this source in detail to give an introduction to the tidync summary. 

```{r basic-raster}
oisstfile <- system.file("nc/reduced.nc", package = "stars")
```

To connect to this file we use `tidync()`. 

```{r basic-raster-connect}
oisst <- tidync(oisstfile)
```

*NB: this is not a real connection, like that used by ncdf4 or RNetCDF - tidync functions always open the file in read-only mode, extract information and/or data, and then close the open file connection.*

To see the available data in the file print a summmary of the source. 

```{r basic-raster-print}
print(oisst)
```

There are three kinds of information 

* one  (1) Data Source, our one file
* five (5) Grids, available *spaces* (or shapes) in the source 
* four (4) Dimensions, orthogonal axes from which Grids are composed

There is only one Grid available for multidimensional data, which is the first one "D0,D1,D2,D3" - all other Grids are one-dimensional. The 4D grid has four variables `sst`, `anom`, `ice`, and `err` and each 1D grid has a single variable. 

*NB: the 1D grids have a corresponding named dimension and name variable, making these "coordinate dimensions" see `coord_dim` in the Dimensions table, it's not necessarily true that a 1D grid will have a single 1D variable, it may have more than one variable, and it may only have an "index variable", i.e. no data values only the position `1:length(dimension)`.* 

The dimensions label, name, length, min and max value are seen in the Dimensions table and these values can never change, also see flags `unlim` (an unlimited dimension?) and `coord_dim`. 

The other Dimensions columns `start`, `count`, `dmin`, `dmax` apply when we slice into data variables with `hyper_filter()`. 

### Relationship to ncmeta

Tidync relies on the package [ncmeta](https://CRAN.r-project.org/) to extract information about NetCDF sources. There are functions to find available Grids, Dimensions and Attributes in ncmeta. 

Each grid has a name, size (ndims), and set of variables. Each grid is listed only once, which is an important pattern for each kind of entity when we are programming, the same applies to variables. See that there are 5 grids and 8 variables, with a row for each. 


PLEASE NOTE: some of the code that follows relies on the Github-version of ncmeta to be installed. This can be done with

```R
devtools::install_github("hypertidy/ncmeta")
```

```{r ncmeta-grids}
ncmeta::nc_grids(oisstfile)

ncmeta::nc_vars(oisstfile)
```

Some grids have more than one variable, so they are nested in the grid rows - use unnest to see all variables with their parent grid. 


```{r ncemeta-grids-expandvars}
ncmeta::nc_grids(oisstfile) %>% tidyr::unnest()
```

Similar functions exist for dimensions and variables. 

```{r ncmeta-dimensions-variables}
ncmeta::nc_dims(oisstfile)

ncmeta::nc_atts(oisstfile)
```

There are corresponding functions to find out more about individual variable`, dimensions and attributes by name or by internal index. 

```{r ncmeta-variable1}
ncmeta::nc_var(oisstfile, "anom")
ncmeta::nc_var(oisstfile, 5)

ncmeta::nc_dim(oisstfile, "lon")
ncmeta::nc_dim(oisstfile, 0)

ncmeta::nc_atts(oisstfile)
ncmeta::nc_atts(oisstfile, "zlev")

```

And we can find the internal metadata for each variable by expanding the value. 

```{r ncmeta-time-attributes}
ncmeta::nc_atts(oisstfile, "time") %>% tidyr::unnest()
```

With this information we may now apply the right interpretation to the time values

```{r ncmeta-time-atts}
tunit <- ncmeta::nc_atts(oisstfile, "time") %>% tidyr::unnest() %>% dplyr::filter(name == "units")
RNetCDF::utcal.nc(tunit$value, 1460)

## alternatively we can do this by hand
as.POSIXct("1978-01-01 00:00:00", tz = "UTC") + 1460 * 24 * 3600
```

and check that other independent systems provide the same information. 

```{r raster-stars}
raster::brick(oisstfile, varname = "anom")
stars::read_stars(oisstfile)
```

Tidync is *scared of doing this automatically for you*, but in combination the `ncmeta` package and `tidync` package provides the tools to program around the vagaries presented by NetCDF sources. If you want software that aims to interpret all this for you then check out [stars](https://CRAN.r-project.org/package=stars), [GDAL](https://gdal.org/), [ferret](https://ferret.pmel.noaa.gov/Ferret/), [Panoply](https://www.giss.nasa.gov/tools/panoply/). 


### Degenerate dimensions

See that both `zlev` and `time` are listed as dimensions but have length 1, and also their min and max values are constants. The `zlev` tells us that this grid exists at elevation = 0 (the sea surface) and `time` that the data applies to `time = 1460`, the time is not expressed as a duration (though it presumably applies to the entire day). These are *degenerate dimensions*, i.e. the data is really 2D but we have a record of a 4D space from which they are expressed as a slice. This can cause problems as we would usually treat this data as a matrix in R, and so the `ncdf4` and `RNetCDF` package read functions have arguments that are analogous to R's array indexing argument `drop = TRUE`, if we encounter a dimension of length 1 then drop it. Tidync will also drop dimensions by default when reading data, see `drop` argument in `?hyper_array`. 

### Reading the OISST data

At this point only metadata has been read, so let's read some sea surface temperatures already!

The fastest way to get all the data is to call the function `hyper_array`, this is the lowest level and is very close to using the `ncdf4` or `RNetCDF` package directly. 

```{r read-data}
(oisst_data <- oisst %>% hyper_array())
```

What happened there? We got a classed object, `tidync_data` but this is just a list with arrays. 

```{r oisst-data}
length(oisst_data)
names(oisst_data)
dim(oisst_data[[1]])
image(oisst_data[[1]])
```

This is exactly the data data provided by `ncdf4::ncvar_get()` or `RNetCDF::var.get.nc()` but we can do it in a single line of code. 

```{r oisst-data-single-line}
oisst_data <- tidync(oisstfile) %>% hyper_array()
op <- par(mfrow = n2mfrow(length(oisst_data)))
pals <- c("YlOrRd", "viridis", "Grays", "Blues")
for (i in seq_along(oisst_data)) {
  image(oisst_data[[i]], main = names(oisst_data)[i], col = hcl.colors(20, pals[i], rev = i ==1))
}
par(op)
```

## Transforms

We have done nothing with the spatial side of these data, ignoring the lon and lat values completely. 

```{r oisst-data-dims}
oisst_data

lapply(oisst_data, dim)
```

The print summary of the `oisst_data` object shows that it knows there are four variable and that they each have 2 dimensions (zlev and time were *dropped*), this is now stored as a list of native R arrays, but there is also the transforms attribute available with `hyper_transforms()`. 

The values on each transform table may be used directly. 

```{r oisst-data-transforms}
(trans <- attr(oisst_data, "transforms"))

image(trans$lon$lon, trans$lat$lat,  oisst_data[[1]])
maps::map("world2", add = TRUE)
```


In this case these *transforms* are somewhat redundant, there is a value stored for every step in `lon` and every step in `lat` and they are completely regular, whereas the usual approach in graphics is to store an *offset and scale* rather than each dimension's coordinate. Sometimes though these coordinate values are not reducible this way. 

## Slicing

We can slice into these dimensions using a tidyverse approach. For example, say we wanted to slice out only the data for the waters of the Pacific Ocean, we need a range in longitude and a range in latitude. 

We can put these ranges directly on our raw plot from earlier. 


```{r slicing-long-lat}
lonrange <- c(144, 247)
latrange <- c(-46, 47)

image(trans$lon$lon, trans$lat$lat,  oisst_data[[1]])
#abline(v = lonrange, h = latrange)
rect(lonrange[1], latrange[1], lonrange[2], latrange[2])
```

It's common on the internet to see posts that explain how to drive the NetCDF library with *start* and *count* indices, to do that we need to compare our ranges with the *transforms* of each dimension. 

```{r start-count}
xs <- findInterval(lonrange, trans$lon$lon)
ys <- findInterval(latrange, trans$lat$lat)
print(xs)
print(ys)
start <- c(xs[1], ys[1])
count <- c(diff(xs), diff(ys))

print(start)
print(count)


```

The idea here is that `xs` and `ys` tell us the columns and rows of interest, based on our geographic input in longitude latitude values that we understand. 

Let's try to read with NetCDF.  Hmmm .... what goes wrong. 

```{r read-RNetCDF-fail}
con <- RNetCDF::open.nc(oisstfile)
try(sst_matrix <- RNetCDF::var.get.nc(con, "sst", start = start, count = count))
```

We have been bitten by thinking that this source data is 2D!  So we just add start and count of 1 for each extra dimension (but what if it was 3D, or 5D, or time comes first - all of these things complicate these *simple solutions*). 

```{r read-RNetCDF-succeed}
start <- c(start, 1, 1)
count <- c(count, 1, 1)
sst_matrix <- RNetCDF::var.get.nc(con, "sst", start = start, count = count)

```

And we're good! Except, we now don't have the coordinates for the mapping. We have to slice the lon and lat values as well, but let's cut to the chase and go back to tidync. 

Rather than slice the arrays read into memory, we can *filter* the object that understands the source and it does *not do any data slicing at all*, but records slices *to be done in future*.  This is the lazy beauty of the tidyverse, applied to NetCDF. 

Here I used `between()` for lon and standard R inequality syntax for lat simply to show that both kinds of expression are available. *We don't have to specify the redundant slice into zlev or time*. 

```{r tidync-slice}
library(dplyr)
oisst_slice <- oisst %>% hyper_filter(lon = between(lon, lonrange[1], lonrange[2]), 
                       lat = lat > latrange[1] & lat <= latrange[2])

oisst_slice
```


The print summary has updated the `start` and `count` columns now to match our labouriously acquired versions above (they are slightly different because of the difference between `findInterval()` and our inequality expressions). 

The `dmin` and `dmax` (data-min, data-max) columns are also updated, reporting the coordinate value at the start and end of the slice we have specified. 

Now we can break the lazy chain and call for the data. 

```{r hyper-array}
oisst_slice_data <- oisst_slice %>% hyper_array()
trans <- attr(oisst_slice_data, "transforms")
```

One unfortunate issue here is that we cannot use the transforms directly, they *have* been updated but not in the obvious way (this is something that should probably be fixed in tidync). 

First filter the lon and lat transforms based on the `selected` column. 

```{r hyper-array-slice}
lon <- trans$lon %>% dplyr::filter(selected)
lat <- trans$lat %>% dplyr::filter(selected)

image(lon$lon, lat$lat, oisst_slice_data[[1]])
maps::map("world2", add = TRUE)
```

It is laborious to work with `hyper_array()` but it does give total control over what we can get. 

It's much easier to use other output types. 

```{r tbl-cube}
tcube <- tidync(oisstfile) %>% 
  hyper_filter(lon = between(lon, lonrange[1], lonrange[2]), 
                       lat = lat > latrange[1] & lat <= latrange[2]) %>% 
  hyper_tbl_cube()

library(ggplot2)
ggplot(as_tibble(tcube)) + geom_raster(aes(lon, lat, fill = sst))
```

For those that prefer old-school ggplot2 we can read our slice in directly as a tibble. 

```{r geom_raster}
tdata <- tidync(oisstfile) %>% 
  hyper_filter(lon = between(lon, lonrange[1], lonrange[2]), 
                       lat = lat > latrange[1] & lat <= latrange[2]) %>% 
  hyper_tibble()

ggplot(tdata, aes(lon, lat, fill = anom)) + geom_raster()
```

By default, all variables are available but we can limit with `select_var`. 


```{r select-var}
tidync(oisstfile) %>% 
  hyper_filter(lon = between(lon, lonrange[1], lonrange[2]), 
                       lat = lat > latrange[1] & lat <= latrange[2]) %>% 
  hyper_tibble(select_var = c("err", "ice"))

```

```{r time-series}
tos <- tidync(system.file("nc/tos_O1_2001-2002.nc", package = "stars"))
library(dplyr)
stos <- tos %>% hyper_filter(lon = between(lon, 140, 220), 
                     lat = between(lat, -60, 0)) %>% hyper_tibble()

library(ggplot2)
ggplot(stos, aes(lon, lat, fill = tos)) + geom_raster() + facet_wrap(~time)

```

## Future helpers 

A feature being considered for an upcoming version is to expand out all available linked coordinates. This occurs when an array has a dimension but only stores its index. When a dimension stores values directly this is known as a *dim-coord*, and usually occurs for time values. One way to expand this out would be to include an `expand_coords` argument to `hyper_tibble()` and have it run the following code: 

```{r internal-expand}
#' Expand coordinates stored against dimensions
#'
#' @param x tidync object
#' @param ... ignored
#'
#' @return data frame of all variables and any linked-coordinates 
#' @noRd
#'
#' @examples

full_expand <- function(x, ...) {
  ad <- active(x)
  spl <- strsplit(ad, ",")[[1L]]
  out <- hyper_tibble(x)
  
  for (i in seq_along(spl)) {
    out <- dplyr::inner_join(out, activate(x, spl[i]) %>% hyper_tibble())
  } 
  out
}
```

It's not clear to me how consistently this fits in the wider variants found in the NetCDF world, so any feedback is welcome. 

A real world example is available in the `ncdfgeom` package. This package provides much more in terms of storing geometry within a NetCDF file, but here we only extract the lon, lat and station name that `hyper_tibble()` isn't seeing by default. 

```{r ncdfgeom-example}
huc <- system.file('extdata','example_huc_eta.nc', package = 'ncdfgeom')

full_expand(tidync(huc))

hyper_tibble(tidync(huc))
```



## Thanks!

The `tidync` package recently hit CRAN after a fairly long review process on [rOpenSci](https://github.com/ropensci/software-review/issues/174). In early 2018 I just wasn't sure if it was really going to be wrapped up in a neat way, but thanks to very helpful reviewers and also some key insights about [obscure types](https://github.com/ropensci/tidync/issues/75#issuecomment-468064627) it was done. 

The package benefitted greatly from feedback provided by [Jakub Nowosad](https://github.com/Nowosad) and [Tim Lucas](https://github.com/timcdlucas). 



